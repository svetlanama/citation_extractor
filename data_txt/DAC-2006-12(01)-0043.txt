Exploiting Forwarding to Improve Data Bandwidth ofInstruction-Set Extensions
Ramkumar Jayaseelan , Haibin Liu , Tulika Mitra
School of Computing, National University of Singapore
{ramkumar,liuhb,tulika}@comp.nus.edu.sg
ABSTRACT
Application-speci?c instruction-set extensions (custom instructions) help embedded processors achieve higher performance. Most custom instructions o?ering signi?cant performance bene?t require multiple input operands. Unfortunately, RISC-style embedded processors are designed to
support at most two input operands per instruction. This
data bandwidth problem is due to the limited number of
read ports in the register ?le per instruction as well as the
?xed-length instruction encoding. We propose to overcome
this restriction by exploiting the data forwarding feature
present in processor pipelines. With minimal modi?cations
to the pipeline and the instruction encoding along with cooperation from the compiler, we can supply up to two additional input operands per custom instruction. Experimental
results indicate that our approach achieves 87?100% of the
ideal performance limit for standard benchmark programs.
Additionally, our scheme saves 25% energy on an average by
avoiding unnecessary accesses to the register ?le.
Categories and Subject Descriptors
C.3 [ Special-purpose and application-based systems ]:
Real-time and embedded systems
General Terms
Performance, Design
Keywords
Instruction-set Extensions, Data Forwarding
1. INTRODUCTION
Application-speci?c instructi on-set extensions, also called
custom instructions, extend the instruction-set architecture
of a base processor [6, 7, 9]. Processors that allow such
extensibility have become popular as they strike the right
balance between challenging performance requirement and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro ?t or commercial advantage and that copies
bear this notice and the full citation on the ?rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci ?c
permission and/or a fee.
DAC 2006, July 24?28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007 ...
$5.00.
0%
10% 20% 30%
40%
50% 60% 70%
Rijn
dael
Sha
B low
fi
s h
Djpeg
Com press
Ndes Bit
c nt
s
Dijks tr
a
Speedup
2-input3-input4-input
Figure 1: Impact of limited input operands on performance speedup of custom instructions.
short time-to-market constraints of embedded systems design. Custom instructions enc apsulate the frequently occurring computation patterns in an application. They are
implemented as custom functional units (CFU) in the datapath of an existing processor core. CFUs improve performance through parallelization and chaining of operations.
Thus custom instructions help simple embedded processors
achieve considerable performance and energy e?ciency. Commercial embedded processor supporting instructionset extensibility, such as Alte ra Nios-II [6] and Tensilica
Xtensa [7], are all RISC-style cores with simple instructions and ?xed-length instruction encoding formats. However, custom instructions typ ically encapsulate quite complex computations. This res ults in a fundamental mismatch
betweenthebaseprocessorcoreandthenewextensions
both in terms of ISA de?nition and micro-architecture. Simple RISC-style instructions use at most two register input
operands and one register output operand. As a result, at
the micro-architectural level, the base processor core supports two register read ports per instruction. Unfortunately,
multiple studies [3, 15] have shown that custom instructions
generally require more than two input operands to achieve
any signi?cant performance gain. Figure 1 plots the speedup
due to custom instructions with at most 2, 3, and 4 input
operands, respectively
1. Clearly, performance drops signi?-
cantly as we restrict the number of input operands per custom instruction. In this paper, we present a novel scheme that exploits the
forwarding logic in processor pipeline to overcome the data
bandwidth limit per custom instruction. Data forwarding,
1Details of the experimental setup are given in Section 5
4.2 
43

also known asregister bypassing , is a standard architectural
method to supply data to a functional unit from internal
pipeline bu?ers rather than from programmer-visible registers. In conventional processors, forwarding is used to resolve data hazard between two in-?ight instructions. We
observe that, in many cases, at least some of the input
operands of a custom instruction are available from the data
forwarding logic. Thus, we leverage on the data forwarding
logic to provide additional input s to the custom instructions.
The key to exploiting such a scheme is of course a compile time check to determine if a speci?c operand can indeed
be obtained from the forwarding logic. However, in the
presence of statically unpredictable events, such as cache
miss for a custom instruction, this cannot be guaranteed
at compile time. As the custom instruction gets delayed,
the instruction supplying the operand may complete execution and leave the pipeline. Therefore, the operand is no
longer available from the forwarding logic. To circumvent
this problem, we propose minimal changes in the pipeline
control hardware to guarantee the availability of the operand
from the forwarding logic under such scenarios. At the same
time, we ensure that the changes do not have any negative
impact on the instruction throughput of the pipeline.
Finally, we need to address the related problem of instruction encoding to support additional operands. Assuming an
instruction format similar to the Altera Nios-II processor [6],
we show that minimal modi?cation to the encoding scheme
can support up to 64 custom instructions each having up to
4 input operands.
2. RELATED WORK
Signi?cant research e?ort has been invested in issues related to instruction-set exte nsions for the past few years.
Most of this e?ort has concentrated on the so called ?design
space exploration? problem to choose an appropriate set of
custom instructions for an application [1, 3, 15, 16]. The
?rst step of this exploration process identi?es a large set of
candidate patterns from the program?s data?ow graph and
their frequencies via pro?ling. Given this library of patterns,
the second step selects a subset to maximize the performance
under constraints on the number of allowed custom instructions and/or the area budget. Limited data bandwidth is one of the key problems in
the implementation of custom i nstructions. This problem
arises because custom instructions normally require more
than two input operands whereas the register ?le provides
only two read ports per instruction. Increasing the number
of read ports to the register ?le is not an attractive option
as the area and power consumption grow cubically with the
numb er of p orts. The Nios-II processor [6] solves this problem by allowing
the custom functional unit (CFU) to read/update either the
architectural register ?le or an internal register ?le. However, additional cycles are wasted to move the operands between the architectural register ?le and the internal register
?le through explicit MOVinstructions. Similarly, the MicroBlaze processor [9] from Xilinx provides dedicated Fast Simplex Link (FSL) channels to move operands to the CFU. It
provides putand getinstructions to transfer operands between the architectural register ?le and the CFU through
FSL channels.
Cong et al. [4, 5] eliminate these explicit transfer of operands
with the help of a shadow register ?le associated with the CFU. Shadow register ?le is similar to the internal register ?le of Nios-II in that they both provide input operands
to the CFUs. However, the ma jor di?erence is that the
shadow registers are updated by normal instructions during
the write back stage. An additional bit in the instruction
encoding decides whether the instruction should write to the
shadow register ?le in addition to the architectural register.
Pozzi et al. [13] suggest an orthogonal approach to relax the register ?le port constraints. Their technique exploits the fact that for a CFU with pipelined datapath, all
the operands may not be required in the ?rst clock cycle.
Therefore, the register accesses by the CFU datapath can
be distributed over multiple cycles. This approach will have
limited performance bene?t if most custom instructions with
multiple operands require single-cycle datapath. Though the previous work [4, 5, 13] improve the data
bandwidth, they do not address the related problems of encoding multiple operands in a ?xed-length instruction format and data hazards.
?Fixed-length and ?xed-position encoding employed in
RISC processors do not provide enough space to encode the additional operands of the custom instructions. Previous works do not discuss the issue of encoding operands for custom instructions. For example, the work by Pozzi et al. [13] still requires a register identi?er corresponding to each input operand of
a custom instruction. Similarly, the work based on
shadow register ?les requires either the shadow register identi?er or an architectural register identi?er for
each input operand of a custom instruction.
? Data hazards occur in a pipeline when the dependent instruction reads the register before the source
instruction writes into it. These are resolved by employing data forwarding as discussed in Section 1. For
a multiple-operand custom instruction, data hazards
can occur on any of the input operands. It is not
clear how data hazards are handled for the additional
operands in case of multi-cycle register reads [13] or
shadow registers [5].
Our work addresses both of these important issues. In addition, our method avoids unnecessary register accesses and
thereby saves energy (see Section 5).
3. PROPOSED ARCHITECTURE
Our proposed architecture exploits data forwarding logic
in the processor pipeline to supply additional operands per
custom instruction. In addition, we require minimal modi?cation of the instruction encoding to specify the additional operands per custom instruction. In this section, we
describe these modi?cations in the processor pipeline and
the instruction encoding. We assume a RISC-style in-order
pipeline that is prevalent in embedded processor architectures with extensibility featu re. For illustration purposes,
we use a simple MIPS-like 5-stage pipeline. However, our
technique can be easily applied to other in-order pipelines.
We begin with a brief review of the data forwarding logic as
it is central to our discussion.
3.1 Data Forwarding
We will illustrate our technique through a simple, 5-stage,
MIPS-style pipeline shown in Figure 3. The ?ve pipeline
44

ADD       R1,   R2,  R3
SUB       R4,   R1,  R6
OR         R7,   R8,  R9
CUST    R10, R4,   R7WB
MEM
EX
ID
IF
SUB
OR
CUST
..
.. ADD
SUB
OR
CUST
.. ADD
SUB
OR
CUST ADD
SUB
OR ADD
SUB ADD
Clock
6 5 4 3 2 1
Figure 2: Illustration of data forwarding for a sequence of instructions.
IF/ID ID/EX EX/MEMMEM/WB
PCI-CacheRegister
FileMUX
M UXD-cacheM UX
Figure 3:Data forwarding in a pipeline.
stages are: instruction fetch ( IF), instruction decode/register
read ( ID), execute ( EX), memory access ( MEM)andwriteback ( WB). Data forwarding orregister bypassing is a common technique used to reduce the impact of data hazards
in pipelines. Consider the execution of the sequence of instructions shown in Figure 2 in a MIPS pipeline (the ?rst
register identi?er of each instruction speci?es the destination operand and the other two specify the source operands).
There is a dependency between the ADDinstruction and the
SUB instruction through register R1.The ADDinstruction
writes the result into the register ?le in clock cycle 5. However, the SUBinstruction reads the register ?le in clock cycle
3 and hence would read a wrong value. This is known as
data hazard in the pipeline. To prevent data hazard, we can
stall the pipeline for two clock cycles till the ADDinstruction
writes register R1. This would result in signi?cant performance degradation. A more e?cient method is to forward
the result of the ADDinstruction to the input of the functional unit before it has been written to the register ?le.
This is based on the observation that the SUBinstruction
requires the input only in clock cycle 4 and the ADDinstruction produces the result at the end of clock cycle 3. Thus,
forwarding avoids pipeline stalls due to data hazards. Figure 3 shows the pipeline with the data forwarding
logic darkened. Forwarding paths are provided from the
EX stage (the latch EX/MEM)andthe MEMstage (the latch
MEM/WB ) to the functional units. Multiplexers are placed
before the functional unit to select the operand either from
the register ?le or from the forwarding paths. Note that
there is no forwarding path from the output of the WBstage.
The hazards in this stage are handled by ensuring that the
register writes happen in the ?rst half of a clock cycle and
the register reads happen in the second half of a clock cycle.
Interested readers can refer to [12] for further details.
We observe that in most cases, the operands of custom instructions are available from the forwarding paths. In Figure
RS1RS2RDOPXOP 0
6 5
17  16
22  21
27  26
31
RS1RS2RDOPDOP 0
6 5
17  16
22  21
27  26
31COP
12  11
Original Encoding
Modified Encoding
Figure 4: Encoding format of custom instructions.
2, the custom instruction CUSTreads both its input operands
from the forwarding path. Hence, the forwarding path can
be used as a proxy to cover up for the lack of number of
read ports in the register ?le. The two latches ( EX/MEMand
MEM/WB ) can provide up to two additional input operands for
a custom instruction (the other two come from the register
?le). Note that in a conventional pipeline, an instruction
reads from the register ?le in the IDstage even if it later
uses the data from the forwarding logic. In contrast, we do
not allow a custom instruction to read from the register ?le
if the corresponding operand will be supplied from the forwarding path. The challenge now is to identify at compile
time which operands will be available from the forwarding
logic, encoding that information in the instruction, and ensuring that the operand is available even in the presence of
unpredictable events (e.g., instruction cache miss).
3.2 Instruction Encoding
We now describe the instruction encoding in the presence
of custom instructions that exploit forwarding logic to obtain up to two additional input operands. The basic idea
behind our encoding is notto a?ect the decoding of normal instructions. We also try to minimize the number of
bits required to encode the operand information. We illustrate our encoding with the instruction format of Nios-II
processor. However, the general idea is applicable to any
RISC-style instruction format. The original encoding in Figure 4 is the format for custom
instructions in Nios-II. It consists of a 6-bit opcode ?eld OP,
which is ?xed at 0x32 for all custom instructions. The 11bit opcode extension ?eld OPXis used to distinguish di?erent
custom instructions. 3-bits from the OPX?eld is used in NiosII to indicate whether each source/destination register refers
to the architectural or the internal register ?le (see Section
2). The rest of the 15-bits are used to specify the two source
and one destination operands. As we do not want to a?ect the encoding of normal instructions, all the information about the operands of the
custom instructions are encoded as part of the 11-bit opcode extension ?eld OPX. Each operand of a CFU can come
either from the two register ports or from one of the two
forwarding paths. However, the number of input operands
of a custom instruction need not be encoded as the datapath of the CFU can ignore the extra inputs. For example, a
3-input custom operation would ignore the fourth operand. Among the four input operands, at most two operands
are speci?ed using the forwarding path. There are C
4
2=6
possibilities for the choice of these two operands among the
four input operands. In addition, for each of the operands
from the forwarding path, we need to specify whether it
comes from the EX/MEMlatch or the MEM/WBlatch. There are
a total of four possibilities in this case and hence the total
45

WB
MEM
EX
ID
IF
ADD
SUB
NOP ADD
SUB
NOP ADD
SUB
NOP ADD
SUB
NOP ADD
SUB
Miss ADD
SUB ADD
CC
7 6 5 4 3 2 1
21 20 19 18SUB
OR
CUST
..
.. ADD
SUB
OR
CUST
.. ADD
SUB
OR
CUST ADD
SUB
OR
ADD        R1,   R2, R3
SUB        R4,   R1, R6
OR          R7,   R8, R9
CUST      R10, R4, R7
Cache miss before OR instruction (15 cycles latency)
WB
MEM
EX
ID
IF
NOP
NOP
NOP
NOP
NOP SUB
NOP
NOP
NOP
NOP ADD
SUB
NOP
NOP
NOP ADD
SUB
NOP
NOP ADD
SUB
Miss ADD
SUB ADD
CC
7 6 5 4 3 2 1
21 20 19 18NOP
OR
CUST
..
.. NOP
NOP
OR
CUST
.. NOP
NOP
NOP
OR
CUST NOP
NOP
NOP
NOP
OR
(A) Normal forwarding (B) Predictable forwarding
Figure 5: Pipeline behavior for I-cache miss
WB
MEM
EX
ID
IF
SUB
MULT
CUST
..
.. ADD
SUB
MULT
CUST
.. ADD
SUB
MULT
CUST
.. ADD
SUB
MULT
CUST
.. ADD
SUB
MULT
CUST ADD
SUB
MULT ADD
SUB ADD
CC
8 7
6 5 4 3 2 1
WB
MEM
EX
ID
IF
NOP
MULT
CUST
..
.. NOP
NOP
MULT
CUST
.. SUB
NOP
MULT
CUST
.. ADD
SUB
MULT
CUST
.. ADD
SUB
MULT
CUST ADD
SUB
MULT ADD
SUB
ADD
CC
8 7
6 5 4 3 2 1(A) Normal forwarding (B) Predictable forwarding
ADD        R1,  R2,  R3
SUB        R4,  R5,  R6
MULT      R7,  R8,  R9
CUST      R10,R4,  R7
Multi-cycle operation MULT (3 cycles)
Figure 6: Pipeline for multi-cycle operation.
number of possibilities that need to be encoded is 24, i.e.,
we require 5 bits to encode the information. The modi?ed
encoding in Figure 4 shows the new instruction format with
the operand information. 5 bits from the OPX?eld is used to
encode the operand information ( OPD?eld). The remaining
6bits( COP?eld) can be used to specify the custom function
to be performed. Thus there can be 64 distinct custom
instructions each having up to four input operands. Note
that decoding the operand information is done in parallel to
the instruction decoding in the IDstage of the pipeline and
hence would not a?ect the cycle time.
3.3 Predictable Forwarding
The key to exploiting forwardi ng for custom instructions
is to determine at compile time (i.e., statically) whether an
operand can be obtained from the forwarding path. In Figure 3 there are two forwarding paths. Let us assume a
sequence of instructions I
1,I2,...In.Anoperandofinstruction I
iis available from the forwarding path only if
instruction I
i  1or Ii  2produces the operand. For example
in Figure 2, the operands of the custom instruction CUSTare
available from the forwarding paths as they are produced by
thetwoimmediatepredecessors( SUBand ORinstructions).
However this property does not hold when there are multicycle operations and also in the event of instruction cache
misses. This is because these events introduce bubbles in the
pipeline and hence a?ect forwarding between dependent instructions. Figure 5(A) illustrates the problem in the event
of an instruction cache miss. The ORinstruction misses in
the instruction cache and hence the custom instruction CUST
cannot obtain the result of the SUBinstruction (register R4)
from the forwarding path. This is not a problem for conven- tional pipeline because normal
instructions will simply read
the result from the register ?le (it was relying on data forwarding only when the result has not yet been written to the
register). Unfortunately, the custom instruction has toread
the data from the forwarding path, i.e., it does not have the
fall back option of reading from the register ?le. Similarly,
multi-cycle operations can a?ect the forwarding path in the
pipeline. Figure 6(A) shows the pipeline behavior when a
multi-cycle instruction MULTis executed in the pipeline.
Data cache misses and branch mispredictions occur in the
MEM stage and hence only the instruction in the WBstage will
not be able to forward the result. As we do not assume
any forwarding from the WBstage anyway (it is taken care
of by split register read/write as discussed in Section 3.1),
branch misprediction and data cache misses do not a?ect
our forwarding path. We suggest a simple change in the pipeline control logic
to guarantee forwarding between instruction I
i  1/Ii  2and
instruction I
iin the event of instruction cache misses and
multi-cycle operations. Events such as cache misses create
bubbles in the pipeline draining out instructions in later
stages of the pipeline. This a?ects forwarding. This can be
clearly seen by comparing Figure 2 with Figure 5(A). In
Figure 2, the SUBinstruction is in the pipeline when the
custom instruction enters the EXstage. However in Figure
5(A), due to the instruction cache miss the SUBinstruction
leaves the pipeline before the custom instruction enters the
EX stage. Therefore the value cannot be forwarded; instead
it should be read from the register ?le in the IDstage.
The key insight is to stall the instructions in the later
stages of the pipeline (after the event) rather than allowing them to progress. That is , instead of introducingNOPs
into the pipeline (as shown i n Figure 5(A)), we retain the
contents of the later stages for the duration of the I-cache
miss. This way when the normal ?ow resumes, the pipeline
looks like as if the event did not happen at all. This scenario is shown in Figure 5(B). Similarly, for multi-cycle
operations the e?ect on the pip eline execution is shown in
Figure 6(B). Note that stalling the pipeline stages as opposed to introducing bubbles (NOPs) does not cause any
additional performance degradation in terms of instruction
throughput. To achieve this, we simply need stall signal for each pipeline
latch. When the stall signal is set, the latch holds its current value. A stall unit is responsible for stalling the pipeline
during cache misses and multi-cycle operations. To ensure
forwarding, the stall signal for latches in the later stages of
thepipelinemustbeset( ID/EX,EX/MEM andMEM/WB )forthe
duration of the cache miss. In a similar fashion the stall
signal for the EX/MEMandMEM/WB latch must be set for the
duration of the multi-cycle operation.
4. COMPILATION TOOLCHAIN
As mentioned before, our technique requires cooperation
from the compiler. We need to determine at compile time
whether a speci?c operand can be forwarded and encode the
custom instruction accordingly. In addition, the compiler
can schedule the instructions appropriately so as to maximize the opportunity of forwarding. We now describe how
these concerns are addressed in the compilation toolchain
for custom instruction selection and exploitation. The relevant portion of the compilation toolchain is shown
46

Pattern Identification
IR Scheduling
Pattern Selection
Register Allocation
Instruction Scheduling 
Forwarding Check
and MOV insertion
Figure 7: Compilation toolchain
in Figure 7. Pattern identi?cation is performed at the intermediate representation (IR) level just prior to register allocation and after the scheduling of the intermediate instructions. We use the pattern identi?cation scheme discussed
in [16] that involves construction of the data dependency
graphs for each basic block followed by identi?cation of all
possible patterns that satisfy the given constraints. In our
case, we impose a constraint that the patterns should have
at most 4 input operands and one output operand. This is
followed by the selection of a subset of patterns to be implement as custom instructions. We now describe the pattern
selection phase in detail.
4.1 Pattern Selection
We use a heuristic pattern selection method. Given the
set of identi?ed patterns, we ?rst club together identical subgraphs using the algorithm presented in [11]. All the identical subgraphs map to a single custom instruction and are
called the instances of a pattern. Associated with each pattern instance, we have an execution count (obtained through
pro?ling) and the speedup. A greedy heuristic method is employed for pattern selection [15]. It attempts to cover each
original instruction in the code with zero or one custom instructions using a priority function given by
Priority
i.j=speedupi.j?frequencyi.j
where Priorityi.j,speedupi.j,and frequencyi.jare the priority, performance speedup and execution frequency of the
j
thinstance of pattern i. The pattern instances are chosen
starting with the highest priority one.
In our forwarding-based approach, the performance speedup
of a pattern instance depends on how many of its input
operands can be forwarded. Suppose we have a 4-input custom instruction. Two of its operands can be obtained from
the forwarding path and two are read from the register ?le.
Then we can easily encode that custom instruction. However, if we cannot obtain any operand from the forwarding
path, then we need to add additional MOVinstructions in the
code. Let us suppose the custom instruction needs three
input operands R2, R3, R4.R2, R3 canbereadfromthe
register ?le. For R4, we insert a redundant instruction MOV
R4, R4 just before the custom instruction. This ensure that
the operand R4can be obtained from the forwarding path.
The latency of a MOVinstruction is one clock cycle. Accordingly, we update the performance speedup of all the custom
instruction instances.
4.2 Instruction Scheduling
The speedup of a custom instruction depends heavily on
the ?nal instruction scheduling (after register allocation) due
BenchmarkSourcePatternsInstances
RijndaelMiBench171790
ShaMiBench1133
Blow?shMiBench13197
DjpegMiBench34133
CompressGothenBurg1126
NdesFSU1339
BitcntsMiBench1128
DijkstraMiBench45
Table 1: Characteristics of benchmark programs.
to the forwarding constraint. Given a basic block with custom instructions, we have formulated the problem of ?nding
the optimal schedule with forwarding as an integer linear
programming (ILP) problem. Due to space constraint, we
do not discuss the formulation in detail here. Interested
readers can refer to [10]. Finally, register allocation (due to register spilling) and
scheduling can a?ect the forwarding paths. Thus the actual encoding of the custom instructions and the insertion
of MOV instructions (if necessary) are performed in the ?nal
stage of the compilation proc ess. The check to determine
if a speci?c input operand can be forwarded is simple. We
just need to check the distance of the custom instruction
from the instruction producing the operand. If the instruction producing the operand spans across basic blocks, then
we have to ensure that the forwarding condition is satis?ed
along all possible program paths.
5. EXPERIMENTAL EVALUATION
In this section we discuss the experimental evaluation of
our proposed architecture.
5.1 Setup
Table 1 shows the characteristics of the benchmark programs selected mostly from MiBench [8]. We use SimpleScalar
tool set [2] for the experiments. The programs are compiled
using gcc 2.7.2.3 with -O3 optimization. Given an application, we ?rst exhaustively enumerate all
possible patterns and their instances [16]. We impose a constraint of maximum 4 input operands and 1 output operand
for any pattern. Table 1 shows the number of patterns and
pattern instances generated for each benchmark. The execution frequencies of the pa ttern instances are obtained
through pro?ling. The hardware latencies and area of custom instructions (patterns) are obtained using Synopsys synthesis tool. Finally, the number of execution cycles of a
custom instruction is computed by normalizing its latency
(rounded up to an integer) against that of a multiply accumulate (MAC) operation, which we assume takes exactly
one cycle. We do not include ?oating-point operations,
memory accesses, and branches in custom instructions as
they introduce non-deterministic behavior. The set of patterns identi?ed is provided as input to the selection phase
which outputs the set of custom instructions selected.
The speedup of an application using custom instructions
is de?ned as follows
S peedup=(Cycle
orig
Cycleex?
1) ?100
where Cycle
origis the number of cycles when the bench47

BenchmarkIdealForwardingMOV
Rijndael64.03%63.85%45.44%
Sha46.82%40.77%20.94%
Blow?sh35.56%35.56%24.38%
Djpeg17.42%17.36%15.43%
Compress26.85%26.85%21.77%
Ndes25.62%22.37%15.55%
Bitcnts20.67%20.67%18.58%
Dijkstra28.99%28.99%19.12%
Table 2: Speedup under di?erent architectures.
mark executes without custom instructions and Cycle
exis
the number of cycles when custom instructions are added.
For the speedup calculations we assume a single issue inorder processor with 100% data cache hit rate.
5.2 Results
We compare the speedup of each benchmark for three different architectures. The ?rst architecture is ?ideal? as it
has support for four read ports in the register ?le and enough
space to encode these operands in the instruction format.
This architecture is able to provide the highest speedup with
custom instructions. The second architecture is based on
our idea of exploiting data ?forwarding?. In this case, we
may need additional MOVinstructions when more than two
operands must be read from the register ?le. The ?nal architecture ?MOV? is based on Nios-II where custom MOV
instructions are used to transfer data from the architectural
register ?le to internal register ?les (see Section 2). Table 2 shows the speedup obtained for the three di?erent architectures. The performance of forwarding is very
close to the ideal performance limit (96% on an average).
This is because for the ma jority of the selected patterns, at
least two operands can be obtained through forwarding; thus
MOV instructions are inserted rarely. The case where custom
move instructions are inserted (?MOV?) achieves only 70%
of the performance limit. Thus our technique can overcome
the limitations in number of register ports and instruction
encoding without a?ecting performance.
In addition, our technique reduces the energy consumption in the register ?le. As data forwarding is predictable
in our approach (refer Section 3.3), register ?le reads can
be avoided for forwarded operands. Table 3 presents the
register ?le energy consumption for the three di?erent architectures. The ?rst column is the ideal case where there
are four read ports in the register ?le. The second column is
the case where there are two read ports and custom MOVinstructions are inserted. The third column is our forwardingbased approach that avoids redundant register ?le accesses.
The energy values presented here are obtained using CACTI
3.2 [14] for 130 nm technology. It is clear from Table 3 that
increasing the number of ports of the register ?le is not an
attractive option as it almost doubles the energy consumption. By comparing the second and third column, it can
be seen that forwarding results in signi?cant savings in the
energy consumption of the register ?le (25% on an average).
6. CONCLUSION AND FUTURE WORK
In this paper we have shown how data forwarding can be
exploited to implement multip le-input single-output (MISO)
BenchmarkEnergy(?J)
IdealMOVForwarding
Rijndael31,46216,39312,373
Sha15,1807,9096,587
Blow?sh30,65315,97212,951
Djpeg3,7211,9391,411
Compress422
Ndes462417
Bitcnts41,68921,72215,915
Dijkstra49,07825,57316,811
Table 3: Register ?le energy consumption under different architectures
custom instructions on a processor with limited number of
register ports. Our technique overcomes the restrictions
imposed by limited register ports and instruction encoding
achieving almost ideal speedup. In the future, we plan to
address restrictions on the number of output operands.
7. ACKNOWLEDGMENTS
This work was partially supported by NUS research grant
R252-000-171-112 and A*STAR Pro ject 022/106/0043.
8. REFERENCES[1] K.Atasu,L.Pozzi,andP.Ienne.Automatic
application-speci?c instruction-set extensions under
microarchitectural constraints. In DAC, 2003.
[2] T. Austin, E. Larson, and D. Ernst. SimpleScalar: An infrastructure for computer system modeling. IEEE
Computer , 35(2), 2002.
[3] N. Clark, H. Zhong, and S. Mahlke. Processor acceleration through automated instruction set customization. In
MICRO , 2003.
[4] J. Cong et al. Instruction set extension with shadow registers for con?gurable processors. In FPGA, 2005.
[5] J. Cong, G. Han, and Z. Zhang. Architecture and compilation for data bandwidth improvement in
con?gurable embedded processors. In ICCAD, 2005.
[6] Altera Corp. Nios processor reference handbook.
[7] R. E. Gonzalez. Xtensa: A con?gurable and extensible processor. IEEE Micro , 20(2), 2000.
[8] M. R. Guthausch et al. Mibench: A free, commercially representative embedded benchmark suite. In IEEE 4th
Annual Workshop on Work load Characterization, 2001.
[9] Xilinx Inc. Microblaze soft processor core.
[10] R. Jayaseelan, H. Liu, and T. Mitra. Exploiting forwarding to improve the data bandwidth of instruction-set
extensions. Technical Report TRB5/06, School of
Computing, National University of Singapore, 2006.
[11] R. Kastner et al. Instruction generation for hybrid recon?gurable systems. ACM Transaction on Design
Automation of Electronic Systems , 7(2), 2002.
[12] D. Paterson and J. Hennessey. Computer Organization and
Design: The Hardware/Software Interface . Morgan
Kaufmann, 3rd edition, 2004.
[13] L. Pozzi and P. Ienne. Exp loiting pipelining to relax
register-?le port constraints of instruction-set extensions. In
CASES , 2005.
[14] P. Shivakumar and N. P. Jouppi. CACTI 3.0: An integrated cache timing, power and area model. Technical
Report 2001/2, Compaq Computer Corporation, 2001.
[15] P. Yu and T. Mitra. Charact erizing embedded applications
for instruction-set extensible processors. In DAC, 2004.
[16] P. Yu and T. Mitra. Scalable custom instructions identi?cation for instruction-set extensible processors. In
CASES , 2004.
48